{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3c1e44-72f7-4641-8c24-f68f85ed56df",
   "metadata": {
    "id": "de3925d8-7b06-4660-a261-6c6122725df7"
   },
   "source": [
    "<h1>F20BC Coursework</h1>\n",
    "<p style=\"font-size:15px;\">This notebook implements and analyses the Biologically-Inspired Computation coursework, focusing on Artificial Neural Networks (ANNs) and Particle Swarm Optimization (PSO). It includes a multi-layer ANN architecture trained using PSO to solve a regression task predicting concrete compressive strength. Additionally, it explores the impact of hyperparameters on model performance through systematic experiments.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebafff1-e6fe-4c34-a1d1-773ddaac79d0",
   "metadata": {},
   "source": [
    "**<p style=\"font-size:18px;\">Data Preparation</p>** \n",
    "This contains all the data preprocessing and cleaning</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a814445-6960-4c14-9e10-edfba438d3a8",
   "metadata": {
    "id": "3a814445-6960-4c14-9e10-edfba438d3a8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f86173b9-c298-4a34-98c2-74a443f8e0a0",
   "metadata": {
    "id": "f86173b9-c298-4a34-98c2-74a443f8e0a0"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset/concrete_data.csv\", skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5650ca39-2c52-45cf-800e-778720fa11d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "5650ca39-2c52-45cf-800e-778720fa11d8",
    "outputId": "fd7be3b5-3708-40c1-b7be-01f4a9dc68c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>blast_furnace_slag</th>\n",
       "      <th>fly_ash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarse_aggregate</th>\n",
       "      <th>fine_aggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>concrete_compressive_strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement  blast_furnace_slag  fly_ash  water  superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   coarse_aggregate  fine_aggregate   age  concrete_compressive_strength  \n",
       "0            1040.0            676.0   28                          79.99  \n",
       "1            1055.0            676.0   28                          61.89  \n",
       "2             932.0            594.0  270                          40.27  \n",
       "3             932.0            594.0  365                          41.05  \n",
       "4             978.4            825.5  360                          44.30  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "033eda81-0b1f-47ab-aee0-fe85ec55ba85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "033eda81-0b1f-47ab-aee0-fe85ec55ba85",
    "outputId": "d40479cf-4067-4002-e741-bc60e7356f19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 9)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the dimensionality of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cca774ba-5993-44d4-b484-6c5d2528a8bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "cca774ba-5993-44d4-b484-6c5d2528a8bd",
    "outputId": "1443dfd3-70e2-4044-b104-72aff9de75ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "concrete_data    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace = True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "08f2c1bb-6ec4-4001-93cb-10c37d52d5d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08f2c1bb-6ec4-4001-93cb-10c37d52d5d3",
    "outputId": "6ad84cb3-0ed9-4a88-f267-3a9055a0cf50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows: 185\n"
     ]
    }
   ],
   "source": [
    "print(f'Duplicate rows: {df.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c15b7ff-dfa1-4597-a5b8-d6ebd235be83",
   "metadata": {
    "id": "2c15b7ff-dfa1-4597-a5b8-d6ebd235be83"
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c161d9-1652-478f-b10f-0d712bcad8cb",
   "metadata": {
    "id": "30f48e29-7d92-4504-b040-9b3f0b83ed87"
   },
   "source": [
    "**<p style=\"font-size:18px;\">Scaling and Encoding</p>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "aad8f012-5e57-418e-ae6e-a48e722ad63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cement  blast_furnace_slag  fly_ash  water  superplasticizer  \\\n",
      "0   540.0                 0.0      0.0  162.0               2.5   \n",
      "1   540.0                 0.0      0.0  162.0               2.5   \n",
      "2   332.5               142.5      0.0  228.0               0.0   \n",
      "3   332.5               142.5      0.0  228.0               0.0   \n",
      "4   198.6               132.4      0.0  192.0               0.0   \n",
      "\n",
      "   coarse_aggregate  fine_aggregate  age  concrete_compressive_strength  \n",
      "0            1040.0           676.0   28                          79.99  \n",
      "1            1055.0           676.0   28                          61.89  \n",
      "2             932.0           594.0  270                          40.27  \n",
      "3             932.0           594.0  365                          41.05  \n",
      "4             978.4           825.5  360                          44.30  \n",
      "Index(['cement', 'blast_furnace_slag', 'fly_ash', 'water', 'superplasticizer',\n",
      "       'coarse_aggregate', 'fine_aggregate', 'age',\n",
      "       'concrete_compressive_strength'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7d2a0134-ad67-4f9a-b445-736cfacd5100",
   "metadata": {
    "id": "7d2a0134-ad67-4f9a-b445-736cfacd5100"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:, :-1] # All columns except the last\n",
    "y = df.iloc[:, -1] # The last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7f4e3c7d-d340-4ef3-a826-b2ead546ddeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1030, 8)\n",
      "(1030,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b8919944-d878-4fed-afee-dbe16561684f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cement  blast_furnace_slag  fly_ash  water  superplasticizer  \\\n",
      "0   540.0                 0.0      0.0  162.0               2.5   \n",
      "1   540.0                 0.0      0.0  162.0               2.5   \n",
      "2   332.5               142.5      0.0  228.0               0.0   \n",
      "3   332.5               142.5      0.0  228.0               0.0   \n",
      "4   198.6               132.4      0.0  192.0               0.0   \n",
      "\n",
      "   coarse_aggregate  fine_aggregate  age  \n",
      "0            1040.0           676.0   28  \n",
      "1            1055.0           676.0   28  \n",
      "2             932.0           594.0  270  \n",
      "3             932.0           594.0  365  \n",
      "4             978.4           825.5  360  \n"
     ]
    }
   ],
   "source": [
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "704c9cfa-9b0f-4ee7-9351-3a75a79c2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c9590511-f8bd-451b-98e2-0e3d519bb67b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9590511-f8bd-451b-98e2-0e3d519bb67b",
    "outputId": "30d15b92-744a-457c-fb6d-f5e987d630ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of scaled training data:\n",
      "[[-0.8284837  -0.85529604  0.76170123 -0.76648792  0.22727321  0.41554504\n",
      "   1.67680283 -0.29298042]\n",
      " [ 0.37482257 -0.85529604 -0.81691314  0.10374823 -1.01399523  1.13697904\n",
      "   0.14190356 -0.63384539]\n",
      " [ 0.31756625  1.56893487 -0.81691314 -1.2347128   1.35268993 -1.55119111\n",
      "   1.35833309 -0.69877204]\n",
      " [ 0.68880886 -0.63854127  1.39785927 -1.31511506  0.78998157 -0.4053092\n",
      "   0.36670573 -0.69877204]\n",
      " [-1.13046463  1.3122516   1.50781251 -0.13272899  2.13055148 -1.73091231\n",
      "  -0.38263485 -0.29298042]]\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 rows of scaled training data:\")\n",
    "print(X_train_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa0bd9-28a9-45cc-9ba4-ad67062e2d75",
   "metadata": {
    "id": "78d90124-ed32-4fe9-a9e9-1be683f411d1"
   },
   "source": [
    "**<p style=\"font-size:18px;\">Artificial Neural Network (ANN)</p>**\n",
    "Below are the implementations of the ANN to train and predict concrete compressive strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f36052af-8785-4f80-a652-a174b844c476",
   "metadata": {
    "id": "f36052af-8785-4f80-a652-a174b844c476"
   },
   "outputs": [],
   "source": [
    "class activationFunction:\n",
    "    def logisticFunction(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def reLuFunction(x):o\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def hyperbolicFunction(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def leakyReLU(x, alpha=0.01):\n",
    "        return np.maximum(x, alpha * x)\n",
    "\n",
    "    def elu(x, alpha=1.0):\n",
    "        return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "class ArtificialNeuralNetwork:\n",
    "    def __init__(self, layerSize, activationFunction):\n",
    "        self.layerSize = layerSize\n",
    "        self.activationFunction = activationFunction\n",
    "        self.weights = [np.random.randn(layerSize[i], layerSize[i + 1]) for i in range(len(layerSize) - 1)]\n",
    "        self.biases = [np.random.randn(1, layerSize[i + 1]) for i in range(len(layerSize) - 1)]\n",
    "\n",
    "    def forwardPropagation(self, x):\n",
    "        output = x\n",
    "        for i in range(len(self.weights)):\n",
    "            matrix_total = np.dot(output, self.weights[i]) + self.biases[i]\n",
    "            output = self.activationFunction[i](matrix_total)  # Apply the ith activation function\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea27674-968b-40b5-8b31-c4de3365a346",
   "metadata": {
    "id": "1ed5052f-91c5-4274-befd-9730ed83beb7"
   },
   "source": [
    "Layer size is structure to be 8,16,8,1 because \n",
    "- 8 input layers\n",
    "- 16 neurons in first hidden layer\n",
    "- 8 for the second hidden layer\n",
    "- 1 for the output layer<p>\n",
    "This is to check the predicted values generated after forward propagation from the ANN.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8c52e988-67b2-445f-ba2a-1b11eabc73d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c52e988-67b2-445f-ba2a-1b11eabc73d8",
    "outputId": "8d0ca147-206a-48fa-ccf8-a9b7b0eac212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN Output (Sample Predictions): [[0.99994023]\n",
      " [0.99962857]\n",
      " [0.98909637]\n",
      " [0.99821397]\n",
      " [0.98765545]\n",
      " [0.99969346]\n",
      " [0.98974584]\n",
      " [0.98997357]]\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [8, 16, 8, 1]\n",
    "\n",
    "activation_functions = [\n",
    "    activationFunction.logisticFunction,\n",
    "    activationFunction.reLuFunction,\n",
    "    activationFunction.hyperbolicFunction,\n",
    "]\n",
    "\n",
    "ann = ArtificialNeuralNetwork(layer_sizes, activation_functions)\n",
    "\n",
    "x_example = np.random.rand(8, 8)  # 8samples, 8 features\n",
    "output = ann.forwardPropagation(x_example)\n",
    "print(\"ANN Output (Sample Predictions):\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d093ea-6808-4968-8508-d6f304cf2453",
   "metadata": {
    "id": "78632050-f421-43b0-ab60-642af572e692"
   },
   "source": [
    "**<p style=\"font-size:18px;\">Loss Function</p>**\n",
    "Since the problem domian is regression, MSE is utilised as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "37370e3e-512a-42a9-a622-35d32b96af49",
   "metadata": {
    "id": "37370e3e-512a-42a9-a622-35d32b96af49"
   },
   "outputs": [],
   "source": [
    "class lossFunction:\n",
    "    def evaluate(self, y_pred, y_train):\n",
    "        pass\n",
    "class MeanSquaredError(lossFunction):\n",
    "    def evaluate(self, y_pred, y_train):\n",
    "        return np.mean((y_pred - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "465dcf79-542a-4fa4-b5de-a4a1da002e8e",
   "metadata": {
    "id": "465dcf79-542a-4fa4-b5de-a4a1da002e8e",
    "outputId": "e4834fcc-a4d2-4aa2-b316-c4e5ccfbcc94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4317911404765424\n"
     ]
    }
   ],
   "source": [
    "loss_function = MeanSquaredError()\n",
    "\n",
    "#output of forward propagation\n",
    "y_pred = ann.forwardPropagation(X_train_scaled)\n",
    "\n",
    "#use the output to calculate the loss function \n",
    "print(\"Loss:\", loss_function.evaluate(y_pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efedc68-995a-4100-a61f-93445bd123fb",
   "metadata": {
    "id": "75222630-f415-4b05-bdf5-778ea94c3c0d"
   },
   "source": [
    "**<p style=\"font-size:18px;\">Implement PSO Algorithm</p>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9917bcd6-8081-4101-9a56-67827043922b",
   "metadata": {
    "id": "9917bcd6-8081-4101-9a56-67827043922b"
   },
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    def __init__(self,vectorSize):\n",
    "            self.particlePosition=np.random.rand(vectorSize)\n",
    "            self.particleVelocity=np.random.rand(vectorSize)\n",
    "            self.bestPosition=np.copy(self.particlePosition)\n",
    "            self.informants=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ff9b7e2d-aa7c-410a-a632-6054362aba9e",
   "metadata": {
    "id": "ff9b7e2d-aa7c-410a-a632-6054362aba9e"
   },
   "outputs": [],
   "source": [
    "def particleToAnn(particle, annLayers, activationFunctions):\n",
    "    neuralNetwork = ArtificialNeuralNetwork(layerSize=annLayers, activationFunction=activationFunctions)\n",
    "    weightBiasIndexCount = 0\n",
    "    \n",
    "    for i in range(len(annLayers) - 1):\n",
    "        prevValue = annLayers[i]\n",
    "        nextValue = annLayers[i + 1]\n",
    "        weightRange = prevValue * nextValue\n",
    "        weight = particle.particlePosition[weightBiasIndexCount:weightBiasIndexCount + weightRange].reshape((prevValue, nextValue))\n",
    "        weightBiasIndexCount += weightRange\n",
    "        biases = particle.particlePosition[weightBiasIndexCount:weightBiasIndexCount + nextValue].reshape((1, nextValue))\n",
    "        weightBiasIndexCount += nextValue\n",
    "        activation = activationFunctions[i]\n",
    "        neuralNetwork.weights[i] = weight\n",
    "        neuralNetwork.biases[i] = biases   \n",
    "    return neuralNetwork\n",
    "    \n",
    "       for i in range(len(annLayers) - 1):\n",
    "        prevValue = annLayers[i]\n",
    "        nextValue = annLayers[i + 1]\n",
    "        weightRange = prevValue * nextValue\n",
    "        weight = particle.particlePosition[weightBiasIndexCount:weightBiasIndexCount + weightRange].reshape((prevValue, nextValue))\n",
    "        weightBiasIndexCount += weightRange\n",
    "        biases = particle.particlePosition[weightBiasIndexCount:weightBiasIndexCount + nextValue].reshape((1, nextValue))\n",
    "        weightBiasIndexCount += nextValue\n",
    "        neuralNetwork.weights[i] = weight\n",
    "        neuralNetwork.biases[i] = biases\n",
    "    return neuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "q8TWIiVxbKSz",
   "metadata": {
    "id": "q8TWIiVxbKSz"
   },
   "outputs": [],
   "source": [
    "# Example of assessFitness\n",
    "'''def assessFitness(particle, X, y, loss_function,predictions):\n",
    "    # Implement forward propagation for the ANN represented by this particle\n",
    "    # Use particle's position as ANN weights/biases\n",
    "    # Calculate the loss (or error) using the provided loss_function\n",
    "    # Return the computed fitness\n",
    "    predictions = particle.forward_prop(X)  # Suppose each particle has a forward_prop method\n",
    "    fitness = loss_function(predictions, y)\n",
    "    return fitness'''\n",
    "\n",
    "def assessFitness(particle,dataset,annLayers,activationFunctions,lossFunction):\n",
    "    x, y = dataset\n",
    "    ann=particleToAnn(particle,annLayers,activationFunctions)\n",
    "    predictions = ann.forward(x.T)\n",
    "    predicted_classes = (predictions > 0.5).astype(int)\n",
    "    accuracy = np.mean(predicted_classes == y.reshape(-1, 1))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "-usoBiIDW1lH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-usoBiIDW1lH",
    "outputId": "da6de4d1-e18e-4d93-c0ee-676ac63d3440"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 17 into shape (16,8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m activationFunctions \u001b[38;5;241m=\u001b[39m [activationFunction\u001b[38;5;241m.\u001b[39mlogisticFunction,activationFunction\u001b[38;5;241m.\u001b[39mreLuFunction]\n\u001b[1;32m      4\u001b[0m particle \u001b[38;5;241m=\u001b[39m Particle(\u001b[38;5;241m161\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m neuralNetwork \u001b[38;5;241m=\u001b[39m \u001b[43mparticleToAnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannLayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivationFunctions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m x_example \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      7\u001b[0m output \u001b[38;5;241m=\u001b[39m neuralNetwork\u001b[38;5;241m.\u001b[39mforwardPropagation(x_example)\n",
      "Cell \u001b[0;32mIn[162], line 8\u001b[0m, in \u001b[0;36mparticleToAnn\u001b[0;34m(particle, annLayers, activationFunctions)\u001b[0m\n\u001b[1;32m      6\u001b[0m nextValue \u001b[38;5;241m=\u001b[39m annLayers[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m weightRange \u001b[38;5;241m=\u001b[39m prevValue \u001b[38;5;241m*\u001b[39m nextValue\n\u001b[0;32m----> 8\u001b[0m weight \u001b[38;5;241m=\u001b[39m \u001b[43mparticle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparticlePosition\u001b[49m\u001b[43m[\u001b[49m\u001b[43mweightBiasIndexCount\u001b[49m\u001b[43m:\u001b[49m\u001b[43mweightBiasIndexCount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweightRange\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevValue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnextValue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m weightBiasIndexCount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weightRange\n\u001b[1;32m     10\u001b[0m biases \u001b[38;5;241m=\u001b[39m particle\u001b[38;5;241m.\u001b[39mparticlePosition[weightBiasIndexCount:weightBiasIndexCount \u001b[38;5;241m+\u001b[39m nextValue]\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m, nextValue))\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 17 into shape (16,8)"
     ]
    }
   ],
   "source": [
    "# Testing the particleToAnn function\n",
    "annLayers = [8, 16, 8, 1]\n",
    "activationFunctions = [activationFunction.logisticFunction,activationFunction.reLuFunction]\n",
    "particle = Particle(161)\n",
    "neuralNetwork = particleToAnn(particle, annLayers, activationFunctions)\n",
    "x_example = np.random.rand(8, 8)\n",
    "output = neuralNetwork.forwardPropagation(x_example)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "hg18WNeDa2Gp",
   "metadata": {
    "id": "hg18WNeDa2Gp"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "class ParticleSwarmOptimisation:\n",
    "    def __init__(self,swarmSize,alpha,beta,delta,omega,jumpSize,informantCount,vectorSize):\n",
    "        self.swarmSize = swarmSize\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta \n",
    "        self.delta = delta\n",
    "        self.omega = omega\n",
    "        self.jumpSize = jumpSize\n",
    "        self.informantCount = informantCount\n",
    "        self.vectorSize = vectorSize\n",
    "        self.global_best = None\n",
    "        self.global_best_fitness = float('inf')\n",
    "        \n",
    "        # assign informants\n",
    "        def initInformants(informantCount,particleArray):\n",
    "            informants=[]\n",
    "            for p in particleArray:\n",
    "                potentialInformants=[]\n",
    "                for potInf in particleArray:\n",
    "                    if potInf!=p:\n",
    "                        potentialInformants.append(potInf)\n",
    "                for i in range(informantCount):\n",
    "                     informants.append(random.choice(potentialInformants))\n",
    "                p.informants=informants\n",
    "\n",
    "        def get_best_informant(particle,dataset,annLayers,activationFunctions,lossFunction):\n",
    "            bestInf=None\n",
    "            bestFitnessInf=float('-inf')\n",
    "            for i in particle.informants:\n",
    "                fitness = assessFitness(i,dataset,annLayers,activationFunctions,lossFunction)\n",
    "                if fitness > bestFitnessInf:\n",
    "                    bestFitnessInf=fitness\n",
    "                    bestInf=i\n",
    "            bestInf.particlePosition=bestFitnessInf\n",
    "            return bestInf.particlePosition\n",
    "\n",
    "        #textbook code\n",
    "        def psoOptimisation(swarmSize,alpha,beta,gamma,jumpSize,informantCount,vectorSize,dataSet):\n",
    "            # stores all of the particles\n",
    "            particleArray=[]\n",
    "            for i in range(swarmSize):\n",
    "                particleArray.append(Particle(vectorSize))\n",
    "            best=None\n",
    "            #initialising informants for the particles\n",
    "            initInformants(informantCount,particleArray)\n",
    "            while(True): #will change to do while loop\n",
    "                # compare fitness\n",
    "                for p in particleArray:\n",
    "                    particleFitness=assessFitness(p)\n",
    "                    bestFitness=assessFitness(best)\n",
    "                    if best is None or particleFitness<bestFitness:\n",
    "                        best=p\n",
    "                for p in particleArray:\n",
    "                    previousBest=p.bestPosition\n",
    "                    informantsBest=get_best_informant\n",
    "                    allBest=best.bestPosition\n",
    "                    b = np.random.uniform(0.0, beta)\n",
    "                    c = np.random.uniform(0.0, gamma)\n",
    "                    d = np.random.uniform(0.0, delta)\n",
    "                    updatedVelocity = alpha * p.velocity + b * (previousBest - p.position) + \\\n",
    "                            c * (informantsBest-p.position) + d * \\\n",
    "                    (allBest - p.position)\n",
    "\n",
    "                    p.velocity=updatedVelocity\n",
    "                    p.position+= jumpSize*updatedVelocity"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
