{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3c1e44-72f7-4641-8c24-f68f85ed56df",
   "metadata": {
    "id": "de3925d8-7b06-4660-a261-6c6122725df7"
   },
   "source": [
    "<h1>F20BC Coursework</h1>\n",
    "<p style=\"font-size:15px;\">This notebook implements and analyses the Biologically-Inspired Computation coursework, focusing on Artificial Neural Networks (ANNs) and Particle Swarm Optimization (PSO). It includes a multi-layer ANN architecture trained using PSO to solve a regression task predicting concrete compressive strength. Additionally, it explores the impact of hyperparameters on model performance through systematic experiments.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebafff1-e6fe-4c34-a1d1-773ddaac79d0",
   "metadata": {},
   "source": [
    "**<p style=\"font-size:18px;\">Data Preparation</p>** \n",
    "This contains all the data preprocessing and cleaning</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "id": "809b53d2-be68-4d76-b10a-080316806117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/imanteh/f20bc/F21BC/Notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "id": "3a814445-6960-4c14-9e10-edfba438d3a8",
   "metadata": {
    "id": "3a814445-6960-4c14-9e10-edfba438d3a8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "id": "f86173b9-c298-4a34-98c2-74a443f8e0a0",
   "metadata": {
    "id": "f86173b9-c298-4a34-98c2-74a443f8e0a0"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Dataset/concrete_data.csv\", skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "id": "5650ca39-2c52-45cf-800e-778720fa11d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "5650ca39-2c52-45cf-800e-778720fa11d8",
    "outputId": "fd7be3b5-3708-40c1-b7be-01f4a9dc68c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>blast_furnace_slag</th>\n",
       "      <th>fly_ash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarse_aggregate</th>\n",
       "      <th>fine_aggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>concrete_compressive_strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement  blast_furnace_slag  fly_ash  water  superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   coarse_aggregate  fine_aggregate   age  concrete_compressive_strength  \n",
       "0            1040.0            676.0   28                          79.99  \n",
       "1            1055.0            676.0   28                          61.89  \n",
       "2             932.0            594.0  270                          40.27  \n",
       "3             932.0            594.0  365                          41.05  \n",
       "4             978.4            825.5  360                          44.30  "
      ]
     },
     "execution_count": 1215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "id": "033eda81-0b1f-47ab-aee0-fe85ec55ba85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "033eda81-0b1f-47ab-aee0-fe85ec55ba85",
    "outputId": "d40479cf-4067-4002-e741-bc60e7356f19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 9)"
      ]
     },
     "execution_count": 1216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the dimensionality of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "id": "cca774ba-5993-44d4-b484-6c5d2528a8bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "cca774ba-5993-44d4-b484-6c5d2528a8bd",
    "outputId": "1443dfd3-70e2-4044-b104-72aff9de75ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cement                           0\n",
       "blast_furnace_slag               0\n",
       "fly_ash                          0\n",
       "water                            0\n",
       "superplasticizer                 0\n",
       "coarse_aggregate                 0\n",
       "fine_aggregate                   0\n",
       "age                              0\n",
       "concrete_compressive_strength    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace = True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "id": "08f2c1bb-6ec4-4001-93cb-10c37d52d5d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08f2c1bb-6ec4-4001-93cb-10c37d52d5d3",
    "outputId": "6ad84cb3-0ed9-4a88-f267-3a9055a0cf50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows: 25\n"
     ]
    }
   ],
   "source": [
    "print(f'Duplicate rows: {df.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "id": "2c15b7ff-dfa1-4597-a5b8-d6ebd235be83",
   "metadata": {
    "id": "2c15b7ff-dfa1-4597-a5b8-d6ebd235be83"
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c161d9-1652-478f-b10f-0d712bcad8cb",
   "metadata": {
    "id": "30f48e29-7d92-4504-b040-9b3f0b83ed87"
   },
   "source": [
    "**<p style=\"font-size:18px;\">Scaling and Encoding</p>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "id": "aad8f012-5e57-418e-ae6e-a48e722ad63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cement  blast_furnace_slag  fly_ash  water  superplasticizer  \\\n",
      "0   540.0                 0.0      0.0  162.0               2.5   \n",
      "1   540.0                 0.0      0.0  162.0               2.5   \n",
      "2   332.5               142.5      0.0  228.0               0.0   \n",
      "3   332.5               142.5      0.0  228.0               0.0   \n",
      "4   198.6               132.4      0.0  192.0               0.0   \n",
      "\n",
      "   coarse_aggregate  fine_aggregate   age  concrete_compressive_strength  \n",
      "0            1040.0            676.0   28                          79.99  \n",
      "1            1055.0            676.0   28                          61.89  \n",
      "2             932.0            594.0  270                          40.27  \n",
      "3             932.0            594.0  365                          41.05  \n",
      "4             978.4            825.5  360                          44.30  \n",
      "Index(['cement', 'blast_furnace_slag', 'fly_ash', 'water', 'superplasticizer',\n",
      "       'coarse_aggregate', 'fine_aggregate ', 'age',\n",
      "       'concrete_compressive_strength'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "id": "7d2a0134-ad67-4f9a-b445-736cfacd5100",
   "metadata": {
    "id": "7d2a0134-ad67-4f9a-b445-736cfacd5100"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:, :-1] # All columns except the last\n",
    "y = df.iloc[:, -1] # The last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "id": "7f4e3c7d-d340-4ef3-a826-b2ead546ddeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1005, 8)\n",
      "(1005,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "id": "b8919944-d878-4fed-afee-dbe16561684f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cement  blast_furnace_slag  fly_ash  water  superplasticizer  \\\n",
      "0   540.0                 0.0      0.0  162.0               2.5   \n",
      "1   540.0                 0.0      0.0  162.0               2.5   \n",
      "2   332.5               142.5      0.0  228.0               0.0   \n",
      "3   332.5               142.5      0.0  228.0               0.0   \n",
      "4   198.6               132.4      0.0  192.0               0.0   \n",
      "\n",
      "   coarse_aggregate  fine_aggregate   age  \n",
      "0            1040.0            676.0   28  \n",
      "1            1055.0            676.0   28  \n",
      "2             932.0            594.0  270  \n",
      "3             932.0            594.0  365  \n",
      "4             978.4            825.5  360  \n"
     ]
    }
   ],
   "source": [
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "id": "704c9cfa-9b0f-4ee7-9351-3a75a79c2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1225,
   "id": "c9590511-f8bd-451b-98e2-0e3d519bb67b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9590511-f8bd-451b-98e2-0e3d519bb67b",
    "outputId": "30d15b92-744a-457c-fb6d-f5e987d630ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of scaled training data:\n",
      "[[-0.7036142   0.7362629  -0.8810702   0.1799759  -1.0226942   1.32341753\n",
      "  -0.17765612 -0.62455318]\n",
      " [-0.83112259 -0.84582676  1.04788182 -0.72981642  0.63596651  1.35948653\n",
      "   0.32102131  0.88513966]\n",
      " [ 0.43140366  1.06620495 -0.8810702   0.38479825 -0.18498677 -1.33280675\n",
      "   0.00745899 -0.28365479]\n",
      " [-1.05233032  0.66864226  1.10026551 -0.30588178  0.28412939  0.42298068\n",
      "  -0.33758549 -0.51092038]\n",
      " [-1.17404287 -0.84582676  1.31904441  0.5419875   0.50193332 -1.24005789\n",
      "   1.18741037 -0.28365479]]\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 rows of scaled training data:\")\n",
    "print(X_train_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa0bd9-28a9-45cc-9ba4-ad67062e2d75",
   "metadata": {
    "id": "78d90124-ed32-4fe9-a9e9-1be683f411d1"
   },
   "source": [
    "**<p style=\"font-size:18px;\">Artificial Neural Network (ANN)</p>**\n",
    "Below are the implementations of the ANN to train and predict concrete compressive strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "id": "f36052af-8785-4f80-a652-a174b844c476",
   "metadata": {
    "id": "f36052af-8785-4f80-a652-a174b844c476"
   },
   "outputs": [],
   "source": [
    "class activationFunction:\n",
    "    def logisticFunction(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def reluFunction(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def hyperbolicFunction(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def leakyReLU(x, alpha=0.01):\n",
    "        return np.maximum(x, alpha * x)\n",
    "\n",
    "    def linearFunction(x):  # Identity function for regression output\n",
    "        return x\n",
    "    \n",
    "    # def elu(x, alpha=1.0):\n",
    "    #     return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "class ArtificialNeuralNetwork:\n",
    "    def __init__(self, layerSize, activationFunction):\n",
    "        self.layerSize = layerSize\n",
    "        self.activationFunction = activationFunction\n",
    "        self.weights = [np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / layer_sizes[i]) for i in range(len(layer_sizes) - 1)]\n",
    "        self.biases = [np.random.randn(1, layer_sizes[i + 1]) for i in range(len(layer_sizes) - 1)]\n",
    "    \n",
    "    def visualize_layers(self):\n",
    "        for i in range(len(self.layerSize)-1):  \n",
    "            print(\"LAYER COUNT:  \", i,\"    NODES: \",self.layerSize[i])\n",
    "            print(i)\n",
    "            print(f\"  Weights Shape:   \", self.weights[i].shape)\n",
    "            print(f\"  Biases Shape:    \",self.biases[i].shape )\n",
    "            print(f\"  Weights= : {self.weights[i]}\")\n",
    "            print(f\"  Biases= {self.biases[i]}\")\n",
    "            \n",
    "    def forwardPropagation(self, x):\n",
    "        output = x\n",
    "        for i in range(len(self.weights)):\n",
    "            matrix_total = np.dot(output, self.weights[i]) + self.biases[i]\n",
    "            output = self.activationFunction[i](matrix_total)  \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea27674-968b-40b5-8b31-c4de3365a346",
   "metadata": {
    "id": "1ed5052f-91c5-4274-befd-9730ed83beb7"
   },
   "source": [
    "Layer size is structure to be 8,16,8,1 because \n",
    "- 8 input layers\n",
    "- 16 neurons in first hidden layer\n",
    "- 8 for the second hidden layer\n",
    "- 1 for the output layer<p>\n",
    "This is to check the predicted values generated after forward propagation from the ANN.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "id": "8c52e988-67b2-445f-ba2a-1b11eabc73d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c52e988-67b2-445f-ba2a-1b11eabc73d8",
    "outputId": "8d0ca147-206a-48fa-ccf8-a9b7b0eac212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER COUNT:   0     NODES:  8\n",
      "0\n",
      "  Weights Shape:    (8, 16)\n",
      "  Biases Shape:     (1, 16)\n",
      "  Weights= : [[-0.5344936   0.46558538 -0.03555715 -0.21315328  0.9239487   0.23647274\n",
      "   0.05400646  0.0673876  -1.54587892  0.32077179  0.30637888  0.40423229\n",
      "  -0.24032778 -0.2127602   1.0165421   0.40449037]\n",
      " [-1.04767111  0.65970553  0.31688568  0.03334888  0.216169   -0.43217275\n",
      "  -0.62654237 -0.32560627  0.66257553 -0.28331339  1.11647225  0.64283512\n",
      "  -0.21887563 -0.23605023  0.08151371  0.13885154]\n",
      " [-0.32680691  0.34771373  1.14817753 -0.44705896 -0.51638874  0.26900766\n",
      "  -0.72443459  0.06521146  0.62465312 -0.07172165  0.31161461 -0.07891479\n",
      "   0.43450633 -1.20678909  0.01857857 -0.45355061]\n",
      " [ 0.54297309  0.10049187  0.50104818  0.47581814 -1.38405805  0.32686663\n",
      "   0.47161121 -0.41028721  0.00572783 -1.05647746 -0.4316316  -0.1413345\n",
      "  -0.1832205  -0.35268209 -0.45748432  0.40191529]\n",
      " [ 0.20205576  0.16599392  0.15648023  1.09135588  0.4259138   0.56651683\n",
      "  -0.29481159  0.42725051  0.47463899  0.13202365  0.27433606  0.21153028\n",
      "  -0.74604888 -0.49768753 -0.39392429  0.21531414]\n",
      " [ 0.28225093 -0.18658877  0.30538884 -0.15621287 -0.79295438  0.48274864\n",
      "   0.07574037  0.29838283  0.54172567 -0.01986144  0.22237888  0.29465535\n",
      "   0.24705813  0.59612579  0.18213387 -0.36651617]\n",
      " [ 0.30065224 -0.01922438 -0.62662764 -0.28386866  0.33091172  0.15845181\n",
      "   0.21379419 -0.53227476  0.43729929 -0.03181681 -1.05290325  0.01600729\n",
      "   0.07782879  0.21634195  0.14075937  0.14202008]\n",
      " [ 0.44757022 -0.30725604 -0.48923055  0.40963619  0.56052884 -0.73686355\n",
      "   0.05498579 -0.87348709 -0.07682008  0.46241212  0.78590854  0.04936397\n",
      "   0.08671791 -0.80895379  0.42299724 -0.43874391]]\n",
      "  Biases= [[-1.0244148   1.26472971 -0.53121772  1.23266191 -0.84903031 -0.55236679\n",
      "   2.38526389  0.2714234   0.06448042  1.57190816 -1.82061253  0.45225302\n",
      "   0.44545134  0.75831239  1.26778752  1.41137817]]\n",
      "LAYER COUNT:   1     NODES:  16\n",
      "1\n",
      "  Weights Shape:    (16, 8)\n",
      "  Biases Shape:     (1, 8)\n",
      "  Weights= : [[ 0.33337348  0.80086806  0.50669568 -0.66562627  0.18406443 -0.46004512\n",
      "  -0.24082616  0.01232445]\n",
      " [ 0.10380013  0.56499017 -0.11598456 -0.10952882 -0.7008262   0.05615852\n",
      "  -0.28085686  0.15853866]\n",
      " [ 0.26943809 -0.17424933  0.1136829  -0.51392534 -0.63102456 -0.31162723\n",
      "  -0.16843174 -0.49044669]\n",
      " [-0.55290936 -0.4830436   0.4910988   0.0097446  -0.27292035 -0.04021545\n",
      "  -0.47579708  0.09573021]\n",
      " [ 0.51386053 -0.10590082  0.0801582  -0.05389892 -0.41477532  0.47648339\n",
      "   0.17983086  0.10658149]\n",
      " [-0.2873982  -0.1652245  -0.19523556  0.22859515  0.03752843  0.02154639\n",
      "   0.47367183  0.17652473]\n",
      " [ 0.07528399 -0.05851032 -0.31188727  0.07619882  0.21931956  0.20601058\n",
      "  -0.34566637  0.6011844 ]\n",
      " [ 0.36526891 -0.17741179 -0.12747774  0.53408076  0.03523647 -0.0661028\n",
      "   0.48735064  0.45099316]\n",
      " [-0.63634874 -0.51928235  0.27065033 -0.40819407 -0.40513052  0.1117753\n",
      "  -0.1497165  -0.22812501]\n",
      " [-0.05218949 -0.04089975 -0.53815092 -0.26337996  0.07261498 -0.18491143\n",
      "  -0.58357568  0.04897762]\n",
      " [-0.2587844  -0.0632153   0.02868952  0.07346006 -0.04280854 -0.03733348\n",
      "   0.64109618 -0.30062388]\n",
      " [ 0.0682311   0.68200038 -0.43604075 -0.03547993  0.31585686  0.2292387\n",
      "   0.18951119  0.09526875]\n",
      " [ 0.21582939  0.03887589  0.2449609   0.02920722 -0.08077478 -0.10361817\n",
      "  -0.27004541  0.5455804 ]\n",
      " [ 0.02136716  0.23793947  0.04354615 -0.50744996  0.05687463 -0.74199073\n",
      "   0.30363828  0.09807373]\n",
      " [ 0.85148011 -0.18948996 -0.78412537 -0.64802466  0.30006693 -0.71650519\n",
      "  -0.0104524  -0.38117634]\n",
      " [-0.17422396  0.09651486 -0.04180662  0.13347158  0.04146369 -0.03872992\n",
      "  -0.29706089 -0.41940426]]\n",
      "  Biases= [[ 1.32427252  1.43912598  0.99886671  1.03388383 -0.68541071  0.73206218\n",
      "  -0.84188514 -0.51426627]]\n",
      "LAYER COUNT:   2     NODES:  8\n",
      "2\n",
      "  Weights Shape:    (8, 1)\n",
      "  Biases Shape:     (1, 1)\n",
      "  Weights= : [[-0.07088991]\n",
      " [ 0.38687234]\n",
      " [ 0.04473612]\n",
      " [-0.46775264]\n",
      " [ 0.75499749]\n",
      " [ 0.35857422]\n",
      " [-0.85782121]\n",
      " [ 0.29579437]]\n",
      "  Biases= [[2.73895799]]\n",
      "ANN Output (Sample Predictions): [[3.19767528]\n",
      " [3.21663167]\n",
      " [3.18305499]\n",
      " [3.20177828]\n",
      " [3.24035614]\n",
      " [3.18801924]\n",
      " [3.2174719 ]\n",
      " [3.26891213]]\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [8, 16, 8, 1]\n",
    "\n",
    "activation_functions = [\n",
    "    activationFunction.logisticFunction,\n",
    "    activationFunction.reluFunction,\n",
    "    activationFunction.linearFunction,\n",
    "]\n",
    "ann = ArtificialNeuralNetwork(layer_sizes, activation_functions)\n",
    "ann.visualize_layers()\n",
    "\n",
    "x_example = np.random.rand(8, 8)  #8 samples, 8 features\n",
    "output = ann.forwardPropagation(x_example)\n",
    "print(\"ANN Output (Sample Predictions):\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d093ea-6808-4968-8508-d6f304cf2453",
   "metadata": {
    "id": "78632050-f421-43b0-ab60-642af572e692"
   },
   "source": [
    "**<p style=\"font-size:18px;\">Loss Function</p>**\n",
    "Since the problem domian is regression, MSE is utilised as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "id": "37370e3e-512a-42a9-a622-35d32b96af49",
   "metadata": {
    "id": "37370e3e-512a-42a9-a622-35d32b96af49"
   },
   "outputs": [],
   "source": [
    "class lossFunction:\n",
    "    def evaluate(self,y_pred,y_train):\n",
    "        self.y_pred=y_pred\n",
    "        self.y_train=y_train\n",
    "class MeanSquaredError(lossFunction):\n",
    "    def evaluate(self, y_pred, y_train):\n",
    "        return np.mean((y_pred - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "id": "465dcf79-542a-4fa4-b5de-a4a1da002e8e",
   "metadata": {
    "id": "465dcf79-542a-4fa4-b5de-a4a1da002e8e",
    "outputId": "e4834fcc-a4d2-4aa2-b316-c4e5ccfbcc94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred shape: (703, 1)\n",
      "y_train shape: (703, 1)\n",
      "Loss: 1264.6379607984675\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "y_pred = ann.forwardPropagation(X_train_scaled) # Forward propagating  through the ANN\n",
    "# checking y_pred and y_train has the correct shape\n",
    "print(\"y_pred shape:\", y_pred.shape) \n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# use to calculate the loss function\n",
    "loss_function=MeanSquaredError()\n",
    "loss=loss_function.evaluate(y_pred, y_train)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7df601-b058-4105-bdd4-8ddd878a28fe",
   "metadata": {
    "id": "75222630-f415-4b05-bdf5-778ea94c3c0d"
   },
   "source": [
    "**<p style=\"font-size:18px;\">Implement PSO Algorithm</p>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "id": "9917bcd6-8081-4101-9a56-67827043922b",
   "metadata": {
    "id": "9917bcd6-8081-4101-9a56-67827043922b"
   },
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    def __init__(self,vectorSize):\n",
    "            self.particlePosition=np.random.rand(vectorSize)\n",
    "            self.particleVelocity=np.random.rand(vectorSize)\n",
    "            self.bestPosition=np.copy(self.particlePosition)\n",
    "            self.informants=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "id": "ff9b7e2d-aa7c-410a-a632-6054362aba9e",
   "metadata": {
    "id": "ff9b7e2d-aa7c-410a-a632-6054362aba9e"
   },
   "outputs": [],
   "source": [
    "def particleToAnn(particle, annLayers, activationFunctions):\n",
    "    neuralNetwork = ArtificialNeuralNetwork(layerSize=annLayers, activationFunction=activationFunctions)\n",
    "    weightBiasIndexCount = 0\n",
    "    for i in range(len(annLayers) - 1):\n",
    "        prevValue = annLayers[i]\n",
    "        nextValue = annLayers[i + 1]\n",
    "        weightRange = prevValue * nextValue\n",
    "        \n",
    "        weight = particle.particlePosition[weightBiasIndexCount:weightBiasIndexCount + weightRange].reshape((prevValue, nextValue))\n",
    "        weightBiasIndexCount += weightRange\n",
    "        \n",
    "        biases = particle.particlePosition[weightBiasIndexCount:weightBiasIndexCount + nextValue].reshape((1, nextValue))\n",
    "        weightBiasIndexCount += nextValue\n",
    "        \n",
    "        # activation = activationFunctions[i]\n",
    "        neuralNetwork.weights[i] = weight\n",
    "        neuralNetwork.biases[i] = biases\n",
    "    return neuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "id": "q8TWIiVxbKSz",
   "metadata": {
    "id": "q8TWIiVxbKSz"
   },
   "outputs": [],
   "source": [
    "def assessFitness(particle, dataset, annLayers, activationFunctions, loss_function):\n",
    "    x, y = dataset\n",
    "    ann = particleToAnn(particle, annLayers, activationFunctions)\n",
    "    predictions = ann.forwardPropagation(x) \n",
    "    loss = loss_function.evaluate(predictions, y.reshape(-1, 1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "id": "hg18WNeDa2Gp",
   "metadata": {
    "id": "hg18WNeDa2Gp"
   },
   "outputs": [],
   "source": [
    "class ParticleSwarmOptimisation:\n",
    "    def __init__(self, swarmSize, alpha, beta, delta, omega, jumpSize, informantCount, vectorSize):\n",
    "        self.swarmSize = swarmSize\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.delta = delta\n",
    "        self.omega = omega\n",
    "        self.jumpSize = jumpSize\n",
    "        self.informantCount = informantCount\n",
    "        self.vectorSize = vectorSize\n",
    "        self.global_best = None\n",
    "        self.global_best_fitness = float('inf')\n",
    "\n",
    "    def initInformants(self, informantCount, particleArray):\n",
    "        for p in particleArray:\n",
    "            potentialInformants = [potInf for potInf in particleArray if potInf != p]\n",
    "            p.informants = [random.choice(potentialInformants) for _ in range(informantCount)]\n",
    "\n",
    "    def get_best_informant(self, particle, dataset, annLayers, activationFunctions, loss_function):\n",
    "        bestInf = None\n",
    "        bestFitnessInf = float('-inf')\n",
    "        for i in particle.informants:\n",
    "            fitness = assessFitness(i, dataset, annLayers, activationFunctions, loss_function)\n",
    "            if fitness > bestFitnessInf:\n",
    "                bestFitnessInf = fitness\n",
    "                bestInf = i\n",
    "        return bestInf.particlePosition\n",
    "\n",
    "    def psoOptimisation(self, swarmSize, alpha, beta, gamma, jumpSize, informantCount, vectorSize,\n",
    "                        dataset, annLayers, activationFunctions, loss_function, max_iterations=100):\n",
    "        \n",
    "        particleArray = [Particle(vectorSize) for _ in range(swarmSize)]\n",
    "        self.initInformants(informantCount, particleArray)\n",
    "\n",
    "        best = None\n",
    "        iteration = 0\n",
    "\n",
    "        while iteration < max_iterations:\n",
    "            # Update global best\n",
    "            for p in particleArray:\n",
    "                particleFitness = assessFitness(p, dataset, annLayers, activationFunctions, loss_function)\n",
    "                if best is None or particleFitness < assessFitness(best, dataset, annLayers, activationFunctions, loss_function):\n",
    "                    best = p\n",
    "\n",
    "            for p in particleArray:\n",
    "                previousBest = p.bestPosition\n",
    "                informantsBest = self.get_best_informant(p, dataset, annLayers, activationFunctions, loss_function)\n",
    "                allBest = best.bestPosition\n",
    "\n",
    "                b = np.random.uniform(0.0, beta)\n",
    "                c = np.random.uniform(0.0, gamma)\n",
    "                d = np.random.uniform(0.0, delta)\n",
    "\n",
    "                updatedVelocity = (\n",
    "                    alpha * p.particleVelocity +\n",
    "                    b * (previousBest - p.particlePosition) +\n",
    "                    c * (informantsBest - p.particlePosition) +\n",
    "                    d * (allBest - p.particlePosition)\n",
    "                )\n",
    "\n",
    "                p.particleVelocity = updatedVelocity\n",
    "                p.particlePosition += jumpSize * updatedVelocity\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        return best.particlePosition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2b4df-2645-4867-8be5-0721ba47838e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ab11ff8-495b-42ee-80f3-84fcfd6e4707",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px;\">Test out PSO Functionality</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c52e7-3b0c-4c31-ac90-72e95d9dfb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "id": "7142ac47-5039-45b7-9b10-452967e23575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimised ANN Parameters: [0.12093577 0.57526869 0.61514954 0.83009576 0.28939801 0.85119601\n",
      " 0.62269397 0.25565307 0.7944192  0.53508049 0.66222201 0.14177281\n",
      " 0.09399338 0.92001664 0.73986493 0.06586319 0.76183357 0.76915725\n",
      " 0.63281595 0.2008563  0.2980001  0.89657968 0.15106033 0.42338253\n",
      " 0.440563   0.19278999 0.1164078  0.43416178 0.22568048 0.03132411\n",
      " 0.50746962 0.72679913 0.63307539 0.10787054 0.9767174  0.07922487\n",
      " 0.52721549 0.7645804  0.25834639 0.91260172 0.47120798 0.19446552\n",
      " 0.76272999 0.74417239 0.42225365 0.92308749 0.3056555  0.72575778\n",
      " 0.60705158 0.84472964 0.96120522 0.274451   0.10815997 0.59327807\n",
      " 0.3447657  0.34530596 0.34452063 0.8492009  0.8247442  0.54680711\n",
      " 0.20690925 0.54102129 0.67700433 0.8327914  0.69212293 0.28398124\n",
      " 0.21925313 0.65018035 0.69822859 0.17472622 0.14126099 0.55658023\n",
      " 0.41115474 0.68156593 0.11765282 0.55502095 0.16643141 0.83809661\n",
      " 0.08454337 0.42556793 0.09497429 0.5493641  0.85843973 0.32955094\n",
      " 0.65096805 0.35201077 0.16765469 0.3042924  0.14150075 0.20933204\n",
      " 0.76718832 0.1324789  0.66420173 0.42116403 0.22546282 0.39607404\n",
      " 0.08634872 0.31855349 0.62826632 0.05329661 0.31793016 0.89434921\n",
      " 0.65060098 0.24917337 0.7305398  0.20171311 0.39428712 0.07858816\n",
      " 0.74673122 0.38822027 0.48846189 0.84590185 0.77804293 0.32564612\n",
      " 0.41144078 0.90814018 0.14899284 0.89747088 0.14964634 0.14237616\n",
      " 0.92640193 0.30364263 0.95934737 0.50986267 0.59131337 0.26828364\n",
      " 0.66231103 0.81567773 0.75042449 0.35357544 0.7155688  0.79760269\n",
      " 0.31609685 0.48931993 0.16351521 0.16190326 0.22572734 0.72267504\n",
      " 0.48094023 0.76680243 0.54649279 0.71159251 0.76007802 0.68549036\n",
      " 0.74806988 0.65257969 0.66732117 0.14078852 0.0982991  0.47915026\n",
      " 0.2801279  0.61702029 0.24986577 0.44120472 0.17521373 0.12369193\n",
      " 0.22914749 0.56032454 0.08380083 0.30750743 0.20858855 0.88728764\n",
      " 0.3837436  0.22800419 0.65665743 0.8202018  0.92395974 0.54423106\n",
      " 0.46347561 0.54093391 0.4641911  0.47825322 0.54149812 0.11945131\n",
      " 0.14624946 0.56683102 0.81656773 0.46755468 0.90655065 0.68339822\n",
      " 0.7467306  0.70681478 0.87258749 0.8765821  0.52705247 0.44018404\n",
      " 0.52865316 0.24695535 0.32848196 0.03211286 0.87852864 0.8393019\n",
      " 0.87478672 0.82949148 0.09787739 0.6453951  0.91760245 0.30397371\n",
      " 0.78804087 0.34975441 0.0805734  0.78219015 0.89774169 0.51562901\n",
      " 0.71148762 0.1126474  0.6840985  0.7387629  0.25513984 0.91245304\n",
      " 0.74830965 0.86318878 0.57708054 0.79827357 0.55557228 0.29366544\n",
      " 0.56476135 0.52298109 0.91448533 0.13983462 0.16291376 0.37295898\n",
      " 0.43454917 0.51126045 0.49723274 0.1046821  0.66415374 0.19081158\n",
      " 0.86625933 0.10989542 0.37955117 0.77576214 0.51344481 0.70322713\n",
      " 0.91982046 0.58288267 0.47520389 0.42960194 0.43752595 0.25225885\n",
      " 0.39825249 0.25098875 0.85352085 0.2095758  0.56082471 0.42579586\n",
      " 0.85189706 0.46364824 0.14157688 0.5961533  0.32628792 0.42607815\n",
      " 0.47229118 0.37560637 0.61386255 0.88439442 0.65282463 0.14614255\n",
      " 0.86181354 0.787944   0.68937298 0.04265148 0.30322942 0.51123156\n",
      " 0.90314431 0.07356615 0.71115779 0.15569499 0.26287506 0.3899295\n",
      " 0.22953421 0.52801685 0.57561825 0.34605758 0.22977456 0.93084688\n",
      " 0.37064448 0.47537784 0.73255723 0.12731961 0.85129936 0.09523064\n",
      " 0.37788691 0.76570178 0.319461   0.78598331 0.17442666 0.0764325\n",
      " 0.82610984]\n"
     ]
    }
   ],
   "source": [
    "# Code to test out PSO functionality\n",
    "X_train_scaled = np.random.rand(100, 8)  # Simulated scaled training features (100 samples, 8 features)\n",
    "y_train = np.random.rand(100, 1)  # Simulated training labels (100 samples, 1 output)\n",
    "\n",
    "# define hyperparameters for PSO \n",
    "swarmSize = 10\n",
    "alpha = 0.5  # inertia weight\n",
    "beta = 0.5   # cognitive parameter\n",
    "delta = 0.5  # social parameter\n",
    "jumpSize = 0.5\n",
    "informantCount = 3\n",
    "vectorSize = sum([(annLayers[i] * annLayers[i + 1]) + annLayers[i + 1] for i in range(len(annLayers) - 1)])  # Total weights + biases\n",
    "max_iterations = 50\n",
    "\n",
    "# initialize the PSO optimiser\n",
    "pso = ParticleSwarmOptimisation(\n",
    "    swarmSize = swarmSize,\n",
    "    alpha = alpha,\n",
    "    beta = beta,\n",
    "    delta = delta,\n",
    "    omega = omega,\n",
    "    jumpSize = jumpSize,\n",
    "    informantCount = informantCount,\n",
    "    vectorSize = vectorSize\n",
    ")\n",
    "\n",
    "# run it into the psoOptimisation\n",
    "best_parameters = pso.psoOptimisation(\n",
    "    swarmSize = swarmSize,\n",
    "    alpha = alpha,\n",
    "    beta = beta,\n",
    "    gamma = delta,  # Using delta as gamma as per your code\n",
    "    jumpSize = jumpSize,\n",
    "    informantCount = informantCount,\n",
    "    vectorSize = vectorSize,\n",
    "    \n",
    "    dataset = (X_train_scaled, y_train),\n",
    "    annLayers = annLayers,\n",
    "    activationFunctions = activationFunctions,\n",
    "    loss_function= MeanSquaredError(),\n",
    "    max_iterations = max_iterations\n",
    ")\n",
    "print(\"Optimised ANN Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d10176-98cc-4710-a683-f0a49d06430c",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px;\">Get Fitness Value</p>\n",
    "The final fitness represents the performance of the ANN after its weights and biases have been optimised by the PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "id": "eda889e9-90ee-433d-a42f-f7c40e047b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness Value: 0.37908212469034924\n"
     ]
    }
   ],
   "source": [
    "# create a dummy particle and assign the optimised parameters\n",
    "optimised_particle = Particle(vectorSize)\n",
    "optimised_particle.particlePosition = best_parameters\n",
    "\n",
    "# convert the optimised particle position back to ANN weights and biases\n",
    "optimised_ann = particleToAnn(\n",
    "    optimised_particle, \n",
    "    annLayers,\n",
    "    activationFunctions\n",
    ")\n",
    "\n",
    "# run through ANN with the optimised parameters\n",
    "fitness_value = assessFitness(\n",
    "    optimised_particle,  # pass the optimised particle for fitness evaluation\n",
    "    dataset = (X_train_scaled, y_train),\n",
    "    annLayers = annLayers,\n",
    "    activationFunctions = activationFunctions,\n",
    "    loss_function = MeanSquaredError()\n",
    ")\n",
    "print(\"Fitness Value:\", fitness_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fb274-eccd-464b-b0d0-eaa166c3c264",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px;\">Training the ANN</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "id": "9bc8ad1a-430d-4e25-90c6-62c7eb4863ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.37908212469034924\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = optimised_ann.forwardPropagation(X_train_scaled)\n",
    "loss_function = MeanSquaredError()\n",
    "training_loss = loss_function.evaluate(y_train_pred, y_train)\n",
    "\n",
    "print(f\"Training Loss: {training_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae53eaa-9653-4c9f-82f1-a48af3361e6b",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px;\">Testing the model on the test set</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1237,
   "id": "62bcc0f9-5453-424e-960f-350ed3c320e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3434949218047184\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = np.random.rand(100, 8)  # Replace with actual test data\n",
    "y_test = np.random.rand(100, 1)  # Replace with actual test labels\n",
    "\n",
    "y_test_pred = optimised_ann.forwardPropagation(X_test_scaled)\n",
    "test_loss = loss_function.evaluate(y_test_pred, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "id": "34c8dfad-0bdf-4c88-9c89-c8433991b135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 2/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 3/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 4/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 5/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 6/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 7/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 8/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 9/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 10/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 11/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 12/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 13/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 14/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 15/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 16/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 17/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 18/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 19/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 20/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 21/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 22/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 23/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 24/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 25/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 26/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 27/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 28/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 29/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 30/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 31/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 32/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 33/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 34/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 35/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 36/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 37/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 38/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 39/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 40/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 41/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 42/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 43/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 44/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 45/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 46/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 47/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 48/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 49/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 50/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 51/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 52/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 53/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 54/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 55/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 56/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 57/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 58/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 59/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 60/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 61/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 62/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 63/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 64/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 65/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 66/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 67/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 68/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 69/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 70/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 71/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 72/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 73/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 74/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 75/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 76/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 77/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 78/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 79/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 80/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 81/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 82/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 83/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 84/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 85/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 86/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 87/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 88/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 89/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 90/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 91/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 92/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 93/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 94/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 95/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 96/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 97/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 98/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 99/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 100/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 101/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 102/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 103/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 104/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 105/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 106/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 107/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 108/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 109/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 110/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 111/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 112/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 113/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 114/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 115/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 116/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 117/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 118/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 119/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 120/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 121/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 122/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 123/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 124/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 125/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 126/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 127/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 128/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 129/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 130/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 131/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 132/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 133/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 134/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 135/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 136/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 137/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 138/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 139/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 140/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 141/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 142/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 143/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 144/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 145/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 146/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 147/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 148/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 149/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 150/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 151/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 152/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 153/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 154/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 155/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 156/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 157/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 158/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 159/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 160/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 161/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 162/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 163/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 164/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 165/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 166/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 167/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 168/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 169/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 170/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 171/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 172/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 173/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 174/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 175/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 176/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 177/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 178/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 179/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 180/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 181/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 182/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 183/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 184/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 185/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 186/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 187/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 188/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 189/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 190/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 191/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 192/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 193/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 194/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 195/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 196/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 197/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 198/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 199/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 200/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 201/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 202/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 203/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 204/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 205/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 206/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 207/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 208/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 209/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 210/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 211/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 212/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 213/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 214/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 215/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 216/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 217/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 218/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 219/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 220/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 221/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 222/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 223/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 224/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 225/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 226/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 227/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 228/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 229/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 230/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 231/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 232/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 233/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 234/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 235/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 236/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 237/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 238/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 239/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 240/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 241/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 242/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 243/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 244/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 245/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 246/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 247/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 248/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 249/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 250/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 251/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 252/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 253/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 254/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 255/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 256/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 257/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 258/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 259/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 260/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 261/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 262/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 263/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 264/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 265/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 266/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 267/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 268/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 269/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 270/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 271/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 272/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 273/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 274/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 275/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 276/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 277/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 278/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 279/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 280/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 281/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 282/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 283/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 284/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 285/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 286/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 287/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 288/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 289/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 290/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 291/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 292/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 293/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 294/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 295/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 296/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 297/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 298/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 299/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 300/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 301/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 302/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 303/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 304/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 305/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 306/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 307/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 308/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 309/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 310/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 311/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 312/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 313/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 314/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 315/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 316/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 317/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 318/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 319/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 320/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 321/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 322/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 323/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 324/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 325/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 326/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 327/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 328/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 329/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 330/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 331/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 332/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 333/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 334/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 335/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 336/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 337/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 338/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 339/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 340/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 341/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 342/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 343/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 344/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 345/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 346/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 347/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 348/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 349/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 350/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 351/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 352/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 353/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 354/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 355/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 356/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 357/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 358/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 359/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 360/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 361/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 362/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 363/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 364/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 365/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 366/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 367/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 368/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 369/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 370/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 371/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 372/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 373/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 374/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 375/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 376/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 377/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 378/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 379/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 380/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 381/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 382/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 383/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 384/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 385/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 386/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 387/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 388/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 389/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 390/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 391/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 392/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 393/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 394/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 395/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 396/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 397/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 398/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 399/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 400/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 401/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 402/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 403/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 404/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 405/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 406/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 407/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 408/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 409/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 410/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 411/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 412/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 413/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 414/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 415/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 416/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 417/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 418/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 419/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 420/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 421/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 422/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 423/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 424/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 425/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 426/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 427/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 428/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 429/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 430/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 431/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 432/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 433/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 434/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 435/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 436/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 437/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 438/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 439/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 440/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 441/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 442/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 443/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 444/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 445/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 446/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 447/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 448/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 449/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 450/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 451/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 452/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 453/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 454/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 455/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 456/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 457/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 458/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 459/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 460/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 461/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 462/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 463/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 464/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 465/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 466/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 467/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 468/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 469/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 470/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 471/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 472/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 473/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 474/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 475/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 476/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 477/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 478/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 479/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 480/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 481/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 482/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 483/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 484/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 485/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 486/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 487/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 488/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 489/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 490/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 491/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 492/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 493/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 494/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 495/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 496/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 497/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 498/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 499/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n",
      "Epoch 500/500 - Training Loss: 0.37908212469034924\n",
      "Test Loss: 0.3434949218047184\n"
     ]
    }
   ],
   "source": [
    "epochs = 500  # Set this to the number of epochs you want\n",
    "max_iterations = 100 \n",
    "for epoch in range(epochs):\n",
    "    y_train_pred = optimised_ann.forwardPropagation(X_train_scaled)\n",
    "    \n",
    "    loss_function = MeanSquaredError()\n",
    "    training_loss = loss_function.evaluate(y_train_pred, y_train)\n",
    "\n",
    "    # Print the training loss\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Training Loss: {training_loss}\")\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_test_pred = optimised_ann.forwardPropagation(X_test_scaled)\n",
    "    test_loss = loss_function.evaluate(y_test_pred, y_test)\n",
    "\n",
    "    # Print the test loss\n",
    "    print(f\"Test Loss: {test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
